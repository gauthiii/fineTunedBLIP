## Image Captioning using Hugging Face transformers

Developed an image captioning pipeline leveraging the Hugging Face Transformers library to generate descriptive captions for images using the BLIP (Bootstrapped Language-Image Pretraining) model. Implemented preprocessing workflows with Pillow for image conversion and tokenized inputs using AutoProcessor for compatibility with the model. Leveraged Python libraries such as transformers, Pillow, and PyTorch to design a scalable, automated solution for image-to-text generation. Gained hands-on experience in transformer-based vision-language models and practical applications in image captioning.



![30](https://github.com/user-attachments/assets/df14f68b-d816-4a30-9bb0-fa4056e694bb)
